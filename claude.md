# Agente Hospitalario - Grupo Pedi√°trico

## Claude Rules

### Restricciones
NO sin permiso: refactorizar, renombrar, mover archivos, cambiar l√≥gica, optimizar, eliminar c√≥digo.

### Permitido
Explicar, detectar errores (sin corregir), escribir c√≥digo SOLO si se pide.

### Plan obligatorio antes de cambios
```
1. archivo1.py - cambio A
2. archivo2.py - cambio B (depende 1)
¬øProceder?
```

---

## Stack

**LLM**: Ollama qwen2.5:3b (function calling)
**RAG**: FAISS + sentence-transformers/all-MiniLM-L6-v2 + cosine similarity
**Chunking**: Offline en 2 pasos (DOCX/PDF ‚Üí JSON intermedio ‚Üí JSON final)
**Backend**: FastAPI
**Bot**: n8n + Telegram (webhook HTTPS) O python-telegram-bot (polling)
**Agente**: Function calling con tool consulta_rag
**T√∫nel**: ngrok (para webhook mode)

---

## ‚öôÔ∏è Configuraci√≥n del LLM - NO modificar sin autorizaci√≥n

### Par√°metros del bot por nivel de impacto

Esta tabla ordena TODOS los par√°metros del bot por su impacto en el comportamiento.

| Par√°metro | Impacto | Ubicaci√≥n | Valor Actual | Qu√© Hace |
|-----------|---------|-----------|--------------|----------|
| **system_prompt** | üî¥ CR√çTICO | `backend/app/llm/client.py:73-118` | 40 l√≠neas, 10 casos de uso | Define comportamiento completo del bot: saludos, despedidas, ambig√ºedad, brevedad, obras sociales |
| **user_prompt** | üî¥ CR√çTICO | `backend/app/llm/client.py:120-133` | Instrucciones por pregunta | C√≥mo usar contexto RAG, m√°ximo palabras, terminar con pregunta |
| **historial** | üî¥ CR√çTICO | `scripts/evaluate_conversational_bot.py:697` | ACTIVADO | Mantiene memoria conversacional. Formato: `[{"role": "user/assistant", "content": "..."}]` |
| **pipeline_mode** | üî¥ CR√çTICO | `backend/app/main.py` | PIPELINE (RAG siempre) | PIPELINE: RAG ejecuta siempre. AGENTE: LLM decide si llamar RAG |
| **temperature** | üü° MEDIO | `backend/app/llm/client.py:172` | 0.1 | Control de creatividad. 0.0=determinista, 1.0=creativo. 0.1=muy preciso, menos alucinaciones |
| **num_predict** | üü° MEDIO | `backend/app/llm/client.py:171` | 120 tokens | M√°ximo de respuesta. 120 tokens ‚âà 50 palabras. Cortar respuesta si muy larga |
| **num_ctx** | üü° MEDIO | `backend/app/llm/client.py:170` | 2048 tokens | Ventana contexto. Cu√°nto historial+RAG puede procesar. M√°s alto=m√°s lento |
| **rag_top_k** | üü° MEDIO | `backend/app/rag/retriever.py` | 3 chunks | Cu√°ntos chunks recupera RAG. M√°s chunks=m√°s contexto pero m√°s lento |
| **embedding_model** | üü° MEDIO | `backend/.env` | BAAI/bge-large-en-v1.5 | Modelo para embeddings. Afecta calidad de b√∫squeda RAG |
| **top_k** | üü¢ BAJO | `backend/app/llm/client.py:173` | 20 | Limita opciones de palabras. Menos opciones=m√°s r√°pido, m√°s determinista |
| **top_p** | üü¢ BAJO | `backend/app/llm/client.py:174` | 0.8 | Nucleus sampling. Corta cola de probabilidades. 0.8=conservador |
| **repeat_penalty** | üü¢ BAJO | `backend/app/llm/client.py:175` | 1.2 | Penaliza repeticiones. 1.0=sin penalidad, 1.2=evita repetir palabras |
| **num_thread** | üü¢ BAJO | `backend/app/llm/client.py:176` | 4 | Hilos CPU para paralelizar. Solo afecta velocidad, no comportamiento |
| **rag_filter** | üü¢ BAJO | `backend/app/rag/retriever.py` | obra_social si mencionada | Filtra chunks por obra social. Mejora precisi√≥n si obra social conocida |

**Estado actual del prompt (2026-01-14)**:
- 40 l√≠neas (antes: 45 l√≠neas)
- 10 casos de uso cubiertos:
  1. Saludos (solo primera vez, IGNORA contexto RAG)
  2. Despedidas
  3. Ambig√ºedad (repregunta)
  4. Fuera de scope (clima, deportes, noticias)
  5. Brevedad (m√°x 50 palabras)
  6. M√∫ltiples obras sociales (pedir una a la vez)
  7. Cambio de tema (adaptarse sin confusi√≥n)
  8. Usuario incorrecto (corregir con amabilidad)
  9. Sobre el bot (explicar funci√≥n)
  10. Pide humano (redirigir a bot primero)

**Cambios recientes (2026-01-14)**:
- ‚úÖ Prompt optimizado: 45 ‚Üí 40 l√≠neas
- ‚úÖ Agregada regla: En saludos ‚Üí IGNORA contexto RAG
- ‚úÖ Agregada regla: Fuera de scope ‚Üí mensaje espec√≠fico
- ‚úÖ Prohibido inventar errores pasados ("confusiones anteriores")
- ‚úÖ Solo disculparse si usuario corrige error REAL
- ‚úÖ Brevedad aumentada: 40 ‚Üí 50 palabras (para requisitos completos)

### Problemas detectados en √∫ltima evaluaci√≥n (2026-01-11 15:23)

1. **Saludo menciona ENSALUD sin que nadie lo pidiera** ‚úÖ SOLUCIONADO
   - Causa: RAG recupera chunk de ENSALUD, LLM lo usa incorrectamente
   - Soluci√≥n aplicada: Regla expl√≠cita "En saludos ‚Üí IGNORA contexto RAG"

2. **Bot se disculpa por "confusiones anteriores" inexistentes** ‚úÖ SOLUCIONADO
   - Causa: Historial mal interpretado
   - Soluci√≥n aplicada: "Prohibido inventar errores pasados. Solo disculparse si usuario corrige error REAL"

3. **Pregunta del clima: respuesta inadecuada** ‚úÖ SOLUCIONADO
   - Respuesta anterior: "Lo siento por las confusiones..."
   - Soluci√≥n aplicada: Regla "FUERA DE SCOPE: Clima/deportes/noticias ‚Üí 'Solo respondo enrolamiento del Grupo Pedi√°trico. ¬øEn qu√© puedo ayudarte?'"

4. **Tiempos LLM muy lentos**: 80s promedio (vs 1.8s anterior) ‚è≥ PENDIENTE
   - Causa: Historial activado + contexto largo
   - Impacto: Inaceptable para producci√≥n
   - Soluci√≥n propuesta: Cambiar a modo agente (RAG como herramienta)

### Protocolo antes de modificar par√°metros

1. ‚úÖ Documentar valor actual en este archivo
2. ‚úÖ Explicar raz√≥n del cambio
3. ‚úÖ Ejecutar test corto: `python3 scripts/test_improvements.py` (3 preguntas, 2 min)
4. ‚úÖ Si funciona ‚Üí Ejecutar test completo: `python3 scripts/evaluate_conversational_bot.py` (30 preguntas, 15-20 min)
5. ‚úÖ Comparar m√©tricas antes/despu√©s
6. ‚úÖ Documentar resultado

### Reportes de evaluaci√≥n

Ubicaci√≥n: `reports/conversational_evaluation_YYYY-MM-DD_HHMMSS.txt` y `.json`

**√öltimo reporte**: `reports/conversational_evaluation_2026-01-11_144251.txt`
- 30 preguntas en 3 conversaciones
- M√©tricas: Precisi√≥n, Completitud, Concisi√≥n, Habilidades Conv., Performance
- Estado: 3/4 problemas SOLUCIONADOS con prompt optimizado (pendiente validar con test)

---

## Agente con Function Calling

**Archivo**: `backend/app/llm/client.py`

**M√©todo**: `generate_response_agent(query, historial, rag_callback)`

**Herramienta disponible**:
```python
consulta_rag(obra_social: str, query: str)
# Busca en documentos de ENSALUD/ASI/IOSFA
```

**Par√°metros cr√≠ticos** (L357-358, L398-399):
```python
# Primera llamada (sin RAG)
options={
    'temperature': 0.1,      # Muy determinista, menos alucinaciones
    'num_predict': 40        # Max 15 palabras (~3 tokens/palabra)
}

# Segunda llamada (post-RAG)
options={
    'temperature': 0.1,
    'num_predict': 200       # Respuestas completas despu√©s de RAG (aumentado de 50)
}
```

**System Prompt** (L292-329):
- M√ÅXIMO 15 PALABRAS por respuesta
- SI NO SAB√âS ‚Üí USA consulta_rag OBLIGATORIO
- NUNCA inventes copagos, montos, especialidades
- Si RAG vac√≠o ‚Üí "No tengo esa info. ¬øAlgo m√°s?"

**Protocolo b√°sico (conocimiento built-in)**:
```
Guardia: DNI + credencial (NO orden)
Turno: orden + DNI + credencial
Internaci√≥n: orden autorizada + presupuesto
```

**Obras sociales cargadas**: ENSALUD, ASI, IOSFA
Si preguntan por otra ‚Üí "No tengo [X]. Solo ENSALUD/ASI/IOSFA"

---

## RAG Config

**Pipeline de Chunking Offline (2 pasos)**:

1. **Paso 1**: `scripts/convert_docs_to_json.py`
   - DOCX/PDF ‚Üí JSON intermedio (`*_chunks.json`)
   - Extrae texto, tablas, estructura

2. **Paso 2**: `scripts/clean_chunks_v2.py`
   - JSON intermedio ‚Üí JSON final (`*_FINAL.json`)
   - Limpia, valida, estructura metadata
   - 1 chunk JSON = 1 embedding (sin chunking runtime)

**Indexer** (`app/rag/indexer.py`)
```python
# MIGRADO: Ya no procesa PDF/DOCX en runtime
# Ahora indexa directamente desde *_FINAL.json

def index_from_json(json_path):
    # Lee todos los *_FINAL.json
    # 1 chunk JSON = 1 embedding
    # Preserva tablas completas sin splitear
```

**Datos indexados**:
- ASI: 21 chunks
- ENSALUD: 1 chunk
- IOSFA: 3 chunks
- GRUPO_PEDIATRICO: Protocolo base (NO indexado en RAG, hardcoded en prompt)

**Retriever** (`app/rag/retriever.py`)
```python
# L39: Normaliza query embedding
faiss.normalize_L2(query_embedding)

# L59: Convierte inner product ‚Üí cosine similarity (0-1)
similarity = (distance + 1.0) / 2.0

# L64: Threshold RAG - descarta chunks irrelevantes
if similarity < 0.65:  # 0.65=moderado, 0.8+=muy similar
    continue
```

**Entity Extractor** (`app/rag/entity_extractor.py`)
```python
# L13: Obras sociales (ENSALUD, ASI, IOSFA + otras)

# L116-117: Fuzzy matching typos
if similarity > 0.8:  # Tolera errores ortogr√°ficos
    return value, similarity * 0.8
```

**Embeddings**
- Modelo: sentence-transformers/all-MiniLM-L6-v2
- Dim: 384
- Normalizaci√≥n: L2 (para cosine similarity)

**FAISS**
- IndexFlatIP: Inner Product con embeddings normalizados = cosine similarity
- Rango: -1 (opuestos) a +1 (id√©nticos)
- B√∫squeda exacta, no aproximada

---

## Telegram Integration

### Opci√≥n A: Bot Python directo (`telegram_bot.py`)

**Memoria conversacional**:
```python
from collections import deque
conversation_history = defaultdict(lambda: deque(maxlen=10))
```

**Payload al backend**:
```python
payload = {
    "pregunta": user_message,
    "obra_social": None,
    "historial": list(conversation_history[chat_id]),
    "use_agent": True  # OBLIGATORIO para modo agente
}
```

### Opci√≥n B: n8n + Telegram (Webhook Mode)

**Workflow**: `n8n/workflows/telegram_agente_hospital.json`

**Requisitos**:
- **ngrok**: T√∫nel HTTPS (Telegram requiere HTTPS para webhooks)
- **WEBHOOK_URL**: Variable de entorno para n8n

**Flujo**:
1. Telegram ‚Üí ngrok (HTTPS) ‚Üí n8n webhook
2. n8n ‚Üí HTTP POST a `localhost:8000/query`
3. n8n ‚Üí Env√≠a respuesta a Telegram

**Setup**:
```bash
# Terminal 1: Backend
cd backend && python3 -m uvicorn app.main:app --reload

# Terminal 2: ngrok (obtener URL HTTPS)
cd ~ && ./ngrok http 5678

# Terminal 3: n8n (con URL de ngrok)
export WEBHOOK_URL=https://<ngrok-url>/
n8n start
```

**Nota**: La URL de ngrok cambia cada vez (plan gratuito). Necesitas actualizar `WEBHOOK_URL` cada sesi√≥n.

---

## Archivos Cr√≠ticos

```
backend/app/
‚îú‚îÄ‚îÄ main.py                    # L215-265: Endpoint /query con rag_callback
‚îú‚îÄ‚îÄ models.py                  # L19: use_agent field
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ entity_extractor.py    # L13: OBRAS_SOCIALES
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py           # L64: threshold cosine
‚îÇ   ‚îî‚îÄ‚îÄ indexer.py             # MIGRADO: index_from_json (lee *_FINAL.json)
‚îî‚îÄ‚îÄ llm/client.py              # L276-421: Agente function calling
                               # L399: num_predict=200 (post-RAG)
                               # L317-328: Ejemplos SIN conteo de palabras

data/obras_sociales_json/
‚îú‚îÄ‚îÄ asi/*_FINAL.json           # 21 chunks
‚îú‚îÄ‚îÄ ensalud/*_FINAL.json       # 1 chunk
‚îú‚îÄ‚îÄ iosfa/*_FINAL.json         # 3 chunks
‚îî‚îÄ‚îÄ grupo_pediatrico/*_FINAL.json  # NO indexado (protocolo base en prompt)

scripts/
‚îú‚îÄ‚îÄ convert_docs_to_json.py    # Paso 1: DOCX/PDF ‚Üí JSON intermedio
‚îú‚îÄ‚îÄ clean_chunks_v2.py         # Paso 2: JSON intermedio ‚Üí JSON final
‚îú‚îÄ‚îÄ index_data.py              # Reindexar FAISS desde JSON
‚îî‚îÄ‚îÄ process_all_step1.py       # Procesar todos los docs (paso 1)

n8n/workflows/
‚îî‚îÄ‚îÄ telegram_agente_hospital.json  # Workflow n8n + Telegram webhook

telegram_bot.py                # Bot Python directo (polling mode)
```

---

## Problemas Resueltos

### ‚úÖ Alucinaciones
**Antes**: Inventaba copagos, especialidades, montos
**Soluci√≥n**:
- System prompt: "NUNCA inventes"
- Temperature 0.1
- num_predict 40
- Si RAG vac√≠o ‚Üí "No tengo esa info"

### ‚úÖ Respuestas cortadas post-RAG
**Antes**: Respuestas se cortaban a ~200 caracteres (num_predict=50 muy bajo)
**Soluci√≥n**: num_predict=200 en segunda llamada (post-RAG) - permite respuestas completas

### ‚úÖ LLM muestra conteo de palabras
**Antes**: Bot respond√≠a "DNI, credencial. ¬øQu√© tipo ingreso? (7 palabras)"
**Soluci√≥n**: Eliminado "(X palabras)" de ejemplos en system prompt

### ‚úÖ Confusi√≥n guardia/turno
**Antes**: Dec√≠a "orden" para guardia
**Soluci√≥n**: Prompt expl√≠cito "Guardia: NO orden"

### ‚úÖ Invenci√≥n de obras sociales
**Antes**: Respond√≠a sobre OSDE sin tenerla
**Soluci√≥n**: Lista expl√≠cita + mensaje error

### ‚úÖ Sin memoria conversacional
**Antes**: Cada mensaje sin contexto
**Soluci√≥n**: Deque(maxlen=10) + historial en request

### ‚úÖ Lento (1:53 min/query)
**Antes**: llama3.2 muy lento
**Ahora**: qwen2.5:3b ‚Üí 30-40s queries simples, 180-200s con RAG
**Nota**: Lentitud por CPU sin GPU - no es problema de configuraci√≥n

### ‚úÖ RAG procesa PDF/DOCX en runtime
**Antes**: Indexer chunkeaba documentos en cada indexaci√≥n
**Ahora**: Pipeline offline en 2 pasos ‚Üí JSON ‚Üí indexer lee JSON
**Beneficios**:
- Tablas preservadas completas
- Control humano del chunking
- Reindexaci√≥n m√°s r√°pida

---

## Datos

**Fuentes originales**: `data/obras_sociales/` (PDF/DOCX)
```
ensalud/*.docx
asi/2024-01-04_normas.docx
iosfa/*.docx
docs/checklist_general_grupo_pediatrico.docx
```

**Datos procesados**: `data/obras_sociales_json/` (JSON estructurado)
```
asi/*_FINAL.json           # 21 chunks indexados
ensalud/*_FINAL.json       # 1 chunk indexado
iosfa/*_FINAL.json         # 3 chunks indexados
grupo_pediatrico/*_FINAL.json  # NO indexado (protocolo base)
```

**GRUPO_PEDIATRICO**:
- Protocolo base del hospital (NO es una obra social)
- Aplica a TODOS los pacientes antes de consultar obra social espec√≠fica
- Hardcoded en system prompt del agente
- NO indexado en RAG

**IMPORTANTE**: Documentos NO contienen info sobre copagos espec√≠ficos por especialidad. Si agente dice montos/especialidades ‚Üí est√° ALUCINANDO.

**FAISS Index**: `backend/faiss_index/` (25 documentos totales)

---

## Variables de Entorno

`backend/.env`:
```env
OLLAMA_MODEL=qwen2.5:3b
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
JSON_PATH=data/obras_sociales_json  # Path a JSONs procesados
TOP_K_RESULTS=5

# OBSOLETO (chunking ahora offline):
# CHUNK_SIZE=1000
# CHUNK_OVERLAP=100
# DATA_PATH=data/obras_sociales
# DOCS_PATH=docs
```

---

## Comandos

### Iniciar Sistema (Opci√≥n A: Bot Python)

```bash
# Terminal 1: Backend
cd backend && source venv/bin/activate
python3 -m uvicorn app.main:app --reload

# Terminal 2: Bot
python3 telegram_bot.py
```

### Iniciar Sistema (Opci√≥n B: n8n + ngrok)

```bash
# Terminal 1: Backend
cd backend && source venv/bin/activate
python3 -m uvicorn app.main:app --reload

# Terminal 2: ngrok
cd ~ && ./ngrok http 5678
# Copiar URL HTTPS que aparece

# Terminal 3: n8n
export WEBHOOK_URL=https://<ngrok-url>/
n8n start
# Abrir http://localhost:5678 y activar workflow
```

### Pipeline de Procesamiento

```bash
# Paso 1: DOCX/PDF ‚Üí JSON intermedio
python scripts/convert_docs_to_json.py

# Paso 2: JSON intermedio ‚Üí JSON final
python scripts/clean_chunks_v2.py

# Paso 3: Reindexar FAISS desde JSON
python scripts/index_data.py
```

### Testing

```bash
# Test directo
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"pregunta": "protocolo b√°sico", "use_agent": true}'

# Health check
curl http://localhost:8000/health
```

---

## Debugging

### Ver tool calls
Buscar en logs backend:
```
üîß Tool call: consulta_rag({'obra_social': 'ASI', 'query': '...'})
üìö Ejecutando RAG: ...
```

### Ver respuestas largas
```
üìù Longitud de respuesta: 307 caracteres  ‚Üê MAL (debe ser < 100)
```

### Verificar modelo
```bash
grep OLLAMA_MODEL backend/.env
# Debe ser: qwen2.5:3b
```

---

## M√©tricas de √âxito

- ‚úÖ NO inventa obras sociales/copagos
- ‚úÖ Llama RAG cuando necesita info
- ‚úÖ "No tengo esa info" si RAG vac√≠o
- ‚úÖ Memoria conversacional (10 msgs)
- ‚ö†Ô∏è Respuestas post-RAG: ~400 chars (objetivo: < 100)
- ‚ö†Ô∏è Performance: 30-40s simple, 180-200s con RAG (CPU sin GPU)

---

## Resumen de Cambios Recientes (Enero 2026)

### Pipeline de Chunking Migrado
- **Antes**: Runtime chunking de PDF/DOCX en cada indexaci√≥n
- **Ahora**: Pipeline offline 2 pasos (DOCX/PDF ‚Üí JSON intermedio ‚Üí JSON final)
- **Ventajas**: Tablas preservadas, control humano, reindexaci√≥n r√°pida

### RAG System
- Migrado de `index_documents()` a `index_from_json()`
- 25 chunks indexados (ASI: 21, ENSALUD: 1, IOSFA: 3)
- GRUPO_PEDIATRICO NO indexado (protocolo base en prompt)

### Telegram Integration
- **Opci√≥n A**: Bot Python directo (polling)
- **Opci√≥n B**: n8n + webhook HTTPS (requiere ngrok)
- ngrok configurado: `~/ngrok http 5678`

### Fixes Aplicados
- `num_predict=200` en segunda llamada (respuestas completas post-RAG)
- Eliminado "(X palabras)" de ejemplos en prompt
- GRUPO_PEDIATRICO renombrado y clarificado (NO es obra social)

### Performance
- Queries simples: 30-40s
- Queries con RAG: 180-200s (limitaci√≥n hardware, no config)
- Primera llamada: ~6s
- Segunda llamada (post-RAG): ~179s (bottleneck identificado)

---

## üöÄ Mejoras Futuras (Documentadas 2026-01-15)

### 1. Patrones de Uso (Prioridad: Alta, Dificultad: F√°cil)

**Objetivo**: Saber qu√© preguntan m√°s los usuarios y de qu√© obra social.

**Implementaci√≥n**:
```python
# logs/usage_stats.json
{
  "2026-01-15": {
    "total_queries": 45,
    "por_obra_social": {"ENSALUD": 20, "ASI": 15, "IOSFA": 10},
    "por_tipo": {"protocolo": 25, "mail": 10, "telefono": 5, "copagos": 5}
  }
}
```

**Archivos a modificar**:
- `telegram_bot.py`: Agregar logging estructurado despu√©s de cada respuesta

**Tiempo estimado**: 1 hora

---

### 2. Preguntas Frecuentes (Prioridad: Media, Dificultad: Media)

**Objetivo**: Identificar top 10 preguntas m√°s comunes para optimizar respuestas.

**Implementaci√≥n**:
```python
# logs/frequent_questions.json
{
  "protocolo_internacion": {"count": 50, "ejemplo": "como interno un paciente"},
  "mail_ensalud": {"count": 30, "ejemplo": "dame el mail de ensalud"},
  "copago_consulta": {"count": 20, "ejemplo": "cuanto sale la consulta"}
}
```

**Opciones de clasificaci√≥n**:
1. Keywords simples (m√°s r√°pido, menos preciso)
2. LLM clasifica cada pregunta (m√°s lento, m√°s preciso)
3. Embeddings + clustering (balance)

**Archivos a modificar**:
- `telegram_bot.py`: Clasificar query antes de procesar
- Nuevo archivo: `backend/app/analytics/classifier.py`

**Tiempo estimado**: 2-3 horas

---

### 3. Feedback Autom√°tico (Prioridad: Media, Dificultad: Media)

**Objetivo**: Detectar cuando el bot no respondi√≥ bien para mejorar.

**Se√±ales de feedback negativo**:
- Usuario repregunta lo mismo (no entendi√≥)
- Usuario dice "no", "no me sirvi√≥", "otra cosa"
- Usuario abandona conversaci√≥n sin despedirse

**Se√±ales de feedback positivo**:
- "Gracias", "Perfecto", "Ok"
- Usuario contin√∫a con otra pregunta (flujo normal)

**Implementaci√≥n**:
```python
# logs/feedback.json
[
  {
    "timestamp": "2026-01-15 13:41",
    "chat_id": "7187787641",
    "query": "qu√© es la denuncia?",
    "response": "La denuncia se refiere...",
    "feedback": "negative",  # repregunt√≥ 3 veces
    "resolved": false
  }
]
```

**Archivos a modificar**:
- `telegram_bot.py`: Detectar patrones de repregunta
- Nuevo archivo: `backend/app/analytics/feedback.py`

**Tiempo estimado**: 3-4 horas

---

### 4. Dashboard de Analytics (Prioridad: Baja, Dificultad: Media)

**Objetivo**: Visualizar m√©tricas en tiempo real.

**Opciones**:
1. Script Python que genera reporte diario
2. Endpoint FastAPI `/analytics` que devuelve JSON
3. Dashboard web simple (Streamlit o similar)

**Tiempo estimado**: 4-6 horas

---

### Orden de Implementaci√≥n Sugerido

1. ‚úÖ Reducir prompt (HECHO)
2. ‚úÖ Reinicio de charlas (HECHO)
3. ‚è≥ Patrones de uso (pr√≥ximo sprint)
4. ‚è≥ Preguntas frecuentes
5. ‚è≥ Feedback autom√°tico
6. ‚è≥ Dashboard
