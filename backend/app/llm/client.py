"""
Cliente para interactuar con Ollama (LLM local)
"""
import ollama
import time
from typing import Optional


class OllamaClient:
    """Cliente para generar respuestas con Ollama"""

    def __init__(self, host: str = "http://localhost:11434", model: str = "qwen2.5:3b"):
        """
        Args:
            host: URL del servidor Ollama
            model: Nombre del modelo a usar (qwen2.5:3b para function calling)
        """
        self.host = host
        self.model = model
        self.client = ollama.Client(host=host)

        # Definir herramientas disponibles para el agente
        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "consulta_rag",
                    "description": "Busca informaci√≥n espec√≠fica en los documentos de obras sociales (ENSALUD, ASI, IOSFA). Usa esta herramienta cuando necesites requisitos, procedimientos o informaci√≥n detallada de una obra social espec√≠fica.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "obra_social": {
                                "type": "string",
                                "description": "La obra social sobre la que buscar (ENSALUD, ASI o IOSFA)",
                                "enum": ["ENSALUD", "ASI", "IOSFA"]
                            },
                            "query": {
                                "type": "string",
                                "description": "La consulta espec√≠fica (ej: 'requisitos internaci√≥n', 'autorizaciones cirug√≠a')"
                            }
                        },
                        "required": ["query"]
                    }
                }
            }
        ]

    def is_available(self) -> bool:
        """Verifica si Ollama est√° disponible"""
        try:
            self.client.list()
            return True
        except Exception as e:
            print(f"Ollama no disponible: {e}")
            return False

    def generate_response(self, query: str, context: str, obra_social: Optional[str] = None, historial: list = None) -> str:
        """
        Genera respuesta usando el LLM

        Args:
            query: Pregunta del usuario
            context: Contexto recuperado del RAG
            obra_social: Obra social espec√≠fica (opcional)
            historial: Lista de mensajes previos [{"role": "user/assistant", "content": "..."}]

        Returns:
            Respuesta generada
        """
        if historial is None:
            historial = []
        # Construcci√≥n del prompt - OPTIMIZADO (40 l√≠neas, 10 casos de uso)
        system_prompt = """Asistente Grupo Pedi√°trico - Enrolamiento

üî¥ REGLAS OBLIGATORIAS:

1. SALUDOS: Solo PRIMERA vez ‚Üí "Hola! Soy un asistente del Grupo Pedi√°trico. ¬øEn qu√© puedo ayudarte?"
   Si ya saludaste ‚Üí no repitas saludo | ‚ö†Ô∏è En saludos ‚Üí IGNORA contexto RAG

2. DESPEDIDAS: "Gracias"/"Chau" ‚Üí "De nada! ¬øAlgo m√°s?" o "Hasta luego!"

3. AMBIG√úEDAD: Falta info ‚Üí pregunt√° (ej: "¬øY el tel√©fono?" ‚Üí "¬øDe qu√© obra social?")

4. FUERA DE SCOPE: Clima/deportes/noticias ‚Üí "Solo respondo enrolamiento del Grupo Pedi√°trico. ¬øEn qu√© puedo ayudarte?"

5. BREVEDAD: M√°ximo 50 palabras. Termin√° SIEMPRE con pregunta.

6. M√öLTIPLES OBRAS SOCIALES: "¬øASI e IOSFA?" ‚Üí "Pregunt√° una obra social a la vez. ¬øCu√°l primero?"

7. CAMBIO DE TEMA: Si el usuario cambia de obra social ‚Üí adaptate sin confusi√≥n

8. USUARIO INCORRECTO: Si dice algo mal ‚Üí correg√≠ con amabilidad

9. SOBRE EL BOT: "¬øC√≥mo funcion√°s?" ‚Üí "Soy asistente del Grupo Pedi√°trico para enrolamiento de ENSALUD/ASI/IOSFA. ¬øQu√© necesit√°s?"

10. PIDE HUMANO: "Quiero hablar con persona" ‚Üí "Puedo ayudarte con enrolamiento. ¬øQu√© necesit√°s?"

üè• OBRAS SOCIALES: ENSALUD, ASI, IOSFA

üìã PROTOCOLO:
‚Ä¢ Consulta: DNI + credencial + validar
‚Ä¢ Pr√°ctica: Lo anterior + orden autorizada
‚Ä¢ Internaci√≥n: Orden + presupuesto + denuncia
‚Ä¢ Guardia: DNI + credencial (sin orden)

‚ö†Ô∏è USO CONTEXTO:
- Si responde la pregunta ‚Üí √∫salo COMPLETO
- Si NO responde ‚Üí ign√≥ralo
- Saludo/despedida/fuera scope ‚Üí ignora contexto

‚ùå PROHIBIDO:
- Inventar errores pasados ("confusiones anteriores")
- Solo disculpate si usuario corrige error REAL
- Inventar datos no en contexto
- Volver a saludar
- Responder ambig√ºedades sin clarificar

Espa√±ol, claro, amable."""

        user_prompt = f"""Contexto disponible:

{context}

---

Pregunta: {query}

INSTRUCCIONES:
1. USA toda la informaci√≥n relevante del contexto
2. Combin√° documentaci√≥n b√°sica + requisitos espec√≠ficos
3. M√°ximo 40 palabras pero SIN OMITIR requisitos importantes
4. Termin√° siempre con pregunta para guiar al usuario
5. Si el contexto no responde la pregunta, decilo claramente"""

        if obra_social:
            user_prompt += f"\n\nNOTA: La consulta es espec√≠ficamente sobre la obra social: {obra_social}"

        try:
            # Construir lista de mensajes incluyendo historial
            messages = [{'role': 'system', 'content': system_prompt}]

            # Agregar historial conversacional (sin incluir el √∫ltimo mensaje del usuario)
            # Filtramos los √∫ltimos 8 mensajes (4 pares user+assistant) para no sobrecargar
            for msg in historial[-8:]:
                # Convertir mensaje (puede ser dict o Pydantic model)
                if hasattr(msg, 'role'):  # Es un objeto Pydantic
                    msg_role = msg.role
                    msg_content = msg.content
                else:  # Es un dict
                    msg_role = msg['role']
                    msg_content = msg['content']

                # No incluir el √∫ltimo mensaje del usuario (ya est√° en user_prompt)
                if msg_role == 'user' and msg_content == query:
                    continue
                messages.append({'role': msg_role, 'content': msg_content})

            # Agregar pregunta actual
            messages.append({'role': 'user', 'content': user_prompt})

            print(f"   ü§ñ Llamando a Ollama (modelo: {self.model})...")
            print(f"   üìä Historial: {len(historial)} mensajes")
            print(f"   üìä Context window: 2048 tokens")

            start_ollama = time.time()
            response = self.client.chat(
                model=self.model,
                messages=messages,
                options={
                    'num_ctx': 2048,       # Contexto suficiente para RAG
                    'num_predict': 120,    # ~50 palabras m√°ximo para respuestas completas
                    'temperature': 0.1,    # Muy determinista = m√°s r√°pido y preciso
                    'top_k': 20,           # Limitar opciones = m√°s r√°pido
                    'top_p': 0.8,          # Nucleus sampling conservador
                    'repeat_penalty': 1.2, # Evitar repeticiones
                    'num_thread': 4        # Paralelizar si tiene CPU multicore
                }
            )
            time_ollama = time.time() - start_ollama

            print(f"   ‚è±Ô∏è  Tiempo de inferencia Ollama: {time_ollama:.3f}s")
            print(f"   üìù Longitud de respuesta: {len(response['message']['content'])} caracteres")

            return response['message']['content']

        except Exception as e:
            return f"Error al generar respuesta: {str(e)}\n\nPor favor verific√° que Ollama est√© corriendo y el modelo '{self.model}' est√© instalado."

    def generate_response_agent(self, query: str, historial: list = None, rag_callback=None) -> dict:
        """
        Genera respuesta usando el agente con function calling

        Args:
            query: Pregunta del usuario
            historial: Lista de mensajes previos
            rag_callback: Funci√≥n callback para ejecutar consulta_rag(obra_social, query)

        Returns:
            dict con {"respuesta": str, "tool_calls": list, "needs_rag": bool}
        """
        if historial is None:
            historial = []

        # System prompt para el agente
        system_prompt = """Asistente Grupo Pedi√°trico.

PROTOCOLO B√ÅSICO:
DNI, credencial, validar, firma, diagn√≥stico.

TIPOS INGRESO:
‚Ä¢ Guardia: DNI + credencial (NO orden)
‚Ä¢ Turno: orden + DNI + credencial
‚Ä¢ Internaci√≥n: orden autorizada + presupuesto

OBRAS SOCIALES: ENSALUD, ASI, IOSFA
Otra obra social ‚Üí "No tengo [X]. Solo ENSALUD/ASI/IOSFA"

üö® REGLAS CR√çTICAS:
1. M√ÅXIMO 15 PALABRAS - si te pas√°s, el sistema falla
2. SI NO SAB√âS ALGO ‚Üí USA consulta_rag OBLIGATORIO
3. NUNCA inventes info (copagos, montos, especialidades)
4. Si no est√° en tus herramientas ‚Üí "No tengo esa info. ¬øNecesit√°s otra cosa?"
5. Termin√° SIEMPRE con pregunta

üîß USA consulta_rag cuando:
- Preguntan detalles de ENSALUD/ASI/IOSFA (circuitos, autorizaciones, requisitos)
- Preguntan info que NO es protocolo b√°sico
- Cualquier duda ‚Üí mejor consultar RAG que inventar

EJEMPLOS CORRECTOS:
User: "protocolo b√°sico"
Bot: DNI, credencial, validar. ¬øQu√© tipo ingreso?

User: "guardia"
Bot: Guardia: DNI + credencial. ¬øObra social?

User: "cu√°nto es copago dermatolog√≠a"
Bot: [USA consulta_rag porque no sab√©s] ‚Üí Si RAG no tiene info ‚Üí No tengo esa info. ¬øAlgo m√°s?

User: "osde"
Bot: No tengo OSDE. Solo ENSALUD/ASI/IOSFA
"""

        # Construir mensajes
        messages = [{'role': 'system', 'content': system_prompt}]

        # Agregar historial
        for msg in historial[-8:]:
            if hasattr(msg, 'role'):
                msg_role = msg.role
                msg_content = msg.content
            else:
                msg_role = msg['role']
                msg_content = msg['content']
            messages.append({'role': msg_role, 'content': msg_content})

        # Agregar pregunta actual
        messages.append({'role': 'user', 'content': query})

        print(f"   ü§ñ Llamando a Ollama AGENTE (modelo: {self.model})...")
        print(f"   üìä Historial: {len(historial)} mensajes")

        try:
            start = time.time()
            response = self.client.chat(
                model=self.model,
                messages=messages,
                tools=self.tools,
                options={
                    'temperature': 0.1,  # M√°s determinista
                    'num_predict': 40  # Forzar 15 palabras m√°ximo (~3 tokens por palabra)
                }
            )
            elapsed = time.time() - start

            print(f"   ‚è±Ô∏è  Tiempo de inferencia: {elapsed:.3f}s")

            message = response['message']

            # Verificar si hay tool calls
            if 'tool_calls' in message and message['tool_calls']:
                tool_call = message['tool_calls'][0]
                function_name = tool_call['function']['name']
                arguments = tool_call['function']['arguments']

                print(f"   üîß Tool call: {function_name}({arguments})")

                # Si hay callback para RAG, ejecutarlo
                if function_name == 'consulta_rag' and rag_callback:
                    obra_social = arguments.get('obra_social')
                    rag_query = arguments.get('query')

                    # Ejecutar RAG
                    print(f"   üìö Ejecutando RAG: obra_social={obra_social}, query={rag_query}")
                    context = rag_callback(obra_social, rag_query)

                    # Llamar de nuevo al LLM con el resultado
                    messages.append(message)
                    messages.append({
                        'role': 'tool',
                        'content': context
                    })

                    # Segunda llamada
                    print(f"   ü§ñ Segunda llamada con resultado de RAG...")
                    start2 = time.time()
                    response2 = self.client.chat(
                        model=self.model,
                        messages=messages,
                        options={
                            'temperature': 0.1,
                            'num_predict': 200  # Respuestas completas despu√©s de RAG
                        }
                    )
                    elapsed2 = time.time() - start2
                    print(f"   ‚è±Ô∏è  Tiempo segunda llamada: {elapsed2:.3f}s")

                    return {
                        "respuesta": response2['message']['content'],
                        "tool_calls": [tool_call],
                        "needs_rag": True
                    }

                return {
                    "respuesta": f"[Herramienta {function_name} requerida pero no disponible]",
                    "tool_calls": [tool_call],
                    "needs_rag": True
                }
            else:
                # No necesita herramientas, respuesta directa
                return {
                    "respuesta": message['content'],
                    "tool_calls": [],
                    "needs_rag": False
                }

        except Exception as e:
            print(f"   ‚ùå Error en agente: {e}")
            return {
                "respuesta": f"Error: {str(e)}",
                "tool_calls": [],
                "needs_rag": False
            }

    def pull_model(self):
        """Descarga el modelo si no est√° disponible"""
        try:
            print(f"Descargando modelo {self.model}...")
            self.client.pull(self.model)
            print(f"‚úÖ Modelo {self.model} descargado correctamente")
        except Exception as e:
            print(f"‚ùå Error descargando modelo: {e}")
