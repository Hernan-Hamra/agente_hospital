# MEJORAS IMPLEMENTADAS - Bot Agente Hospital

**Fecha:** 2026-01-06
**Archivos modificados:** 4
**Estado:** ‚úÖ Listo para probar

---

## 1. ‚úÖ NUEVO SYSTEM PROMPT CON PROTOCOLO B√ÅSICO

### Archivo: `backend/app/llm/client.py`

### Cambios implementados:

#### a) **Lista cerrada de obras sociales**
```
- ENSALUD
- ASI / ASI Salud
- IOSFA
```
‚ùå **Prohibido:** Inventar obras sociales, URLs, tel√©fonos o datos no verificados.

#### b) **Protocolo b√°sico del Grupo Pedi√°trico integrado**
Incluye el checklist general con:
- Documentaci√≥n b√°sica (DNI, credencial, validaci√≥n)
- Protocolos por tipo de ingreso:
  - üè• Ambulatorio/Turnos
  - üö® Guardia
  - üõèÔ∏è Internaci√≥n de urgencia
  - üóìÔ∏è Internaci√≥n programada/cirug√≠a
  - üí∞ Coseguros y exenciones

#### c) **Control de longitud de respuestas**
- **Saludos simples:** Respuesta de 1 l√≠nea
- **Consultas de procedimientos:** Formato estructurado con protocolo b√°sico + espec√≠fico

#### d) **Formato condicional**
- 1Ô∏è‚É£ Saludos ‚Üí corto, sin emojis
- 2Ô∏è‚É£ Sobre el bot ‚Üí breve
- 3Ô∏è‚É£ Obras sociales cargadas ‚Üí lista de 3
- 4Ô∏è‚É£ Procedimientos ‚Üí protocolo b√°sico + requisitos espec√≠ficos
- 5Ô∏è‚É£ Consultas m√©dicas ‚Üí rechazo educado

#### e) **Par√°metros optimizados de Ollama**
```python
options={
    'num_ctx': 2048,      # Contexto reducido (m√°s r√°pido)
    'num_predict': 512,   # Limita longitud de respuesta
    'temperature': 0.3    # Menos hallucinations
}
```

---

## 2. ‚úÖ MEMORIA CONVERSACIONAL

### Archivos modificados:
- `telegram_bot.py`
- `backend/app/models.py`
- `backend/app/main.py`
- `backend/app/llm/client.py`

### Funcionalidad:

#### a) **Almacenamiento en bot de Telegram**
```python
conversation_history = defaultdict(lambda: deque(maxlen=10))
# maxlen=10 = √∫ltimos 5 pares (user + assistant)
```

#### b) **Nuevos comandos**
- `/start` - Inicia conversaci√≥n y limpia historial
- `/clear` - Limpia historial sin reiniciar

#### c) **Integraci√≥n con FastAPI**
- El bot env√≠a `historial: []` en el payload
- FastAPI recibe y pasa al LLM
- LLM usa √∫ltimos 8 mensajes (4 pares) para contexto

#### d) **Prevenci√≥n de duplicados**
El sistema filtra el mensaje actual del historial para evitar duplicaci√≥n en el prompt.

---

## 3. üìä PROBLEMAS RESUELTOS

### Antes ‚Üí Despu√©s

| Problema | Antes | Despu√©s |
|----------|-------|---------|
| **Saludo repetitivo** | 3 p√°rrafos con info no solicitada | 1 l√≠nea: "Hola! Soy un asistente..." |
| **Inventa obras sociales** | Menciona COSYSECO (no existe) | Solo ENSALUD, ASI, IOSFA |
| **Inventa URLs** | `https://ensalud.org/novedades/...` | ‚ùå Prohibido inventar enlaces |
| **Mezcla requisitos** | "Credencial ASI" al hablar de ENSALUD | Protocolo b√°sico separado de espec√≠fico |
| **Sin memoria** | Cada "hola" dispara nueva presentaci√≥n | Recuerda √∫ltimos 5 intercambios |
| **Respuestas muy largas** | Sin l√≠mite | M√°ximo 512 tokens |

---

## 4. üß™ C√ìMO PROBAR LAS MEJORAS

### a) Reiniciar los servicios:

```bash
# Terminal 1: Reiniciar FastAPI
cd /home/hernan/proyectos/agente_hospital/backend
source venv/bin/activate
pkill -f "uvicorn app.main:app"
python3 -m uvicorn app.main:app --reload

# Terminal 2: Reiniciar bot de Telegram
cd /home/hernan/proyectos/agente_hospital
pkill -f "telegram_bot.py"
python3 telegram_bot.py
```

### b) Tests recomendados:

#### Test 1: Saludo simple
```
Usuario: hola
Esperado: Respuesta corta (1-2 l√≠neas), sin formato largo
```

#### Test 2: Memoria conversacional
```
Usuario: hola
Bot: Hola! Soy un asistente...
Usuario: hola
Esperado: El bot NO debe repetir la presentaci√≥n completa
```

#### Test 3: Obras sociales
```
Usuario: ¬øQu√© obras sociales ten√©s?
Esperado: "Tengo informaci√≥n de 3 obras sociales: ENSALUD, ASI e IOSFA."
```

#### Test 4: Obra social no disponible
```
Usuario: Necesito info de OSDE
Esperado: "Actualmente solo tengo informaci√≥n de ENSALUD, ASI e IOSFA."
```

#### Test 5: Protocolo b√°sico + espec√≠fico
```
Usuario: ¬øQu√© documentos necesito para enrolar un paciente de ENSALUD?
Esperado:
- üìã Documentaci√≥n b√°sica (DNI, credencial...)
- üìã Requisitos espec√≠ficos de ENSALUD (del contexto RAG)
```

#### Test 6: Limpiar historial
```
Usuario: hola
Bot: [respuesta]
Usuario: /clear
Bot: üóëÔ∏è Historial de conversaci√≥n limpiado.
Usuario: hola
Esperado: El bot responde como si fuera la primera vez
```

---

## 5. üîç DIAGN√ìSTICO DE VELOCIDAD (Patricia vs Hern√°n)

### Comandos de diagn√≥stico:

Ejecutar estos comandos **en ambas m√°quinas** (Patricia y Hern√°n):

```bash
# 1. Ver info de CPU
lscpu | grep "Model name"
lscpu | grep "CPU(s):"
lscpu | grep "CPU MHz"

# 2. Ver si tienen GPU NVIDIA
nvidia-smi

# 3. Ver modelos Ollama instalados
ollama list

# 4. Ver tama√±o del √≠ndice FAISS
ls -lh /home/hernan/proyectos/agente_hospital/backend/faiss_index/
du -sh /home/hernan/proyectos/agente_hospital/backend/faiss_index/

# 5. Ver cantidad de chunks indexados
# (Al iniciar FastAPI, ver el log: "‚úÖ √çndice cargado: X chunks")

# 6. Monitorear recursos durante una query
# En otra terminal:
htop
# Hacer una consulta y observar:
# - % CPU usado por ollama
# - RAM utilizada
# - Si hay swap
```

### Posibles causas de lentitud:

| Causa | Probabilidad | C√≥mo verificar |
|-------|--------------|----------------|
| **CPU m√°s lenta** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Comparar "Model name" y "CPU MHz" |
| **Sin GPU** | ‚≠ê‚≠ê‚≠ê‚≠ê | `nvidia-smi` - si no tiene, es 10-30x m√°s lento |
| **Modelo diferente** | ‚≠ê‚≠ê‚≠ê | `ollama list` - comparar versiones |
| **M√°s datos indexados** | ‚≠ê‚≠ê | Comparar tama√±o de faiss_index/ |
| **RAM insuficiente** | ‚≠ê‚≠ê | `htop` - ver si usa swap durante query |
| **Modelo embedding no descargado** | ‚≠ê | Solo afecta primera query |

### Soluciones si Hern√°n es m√°s lento:

1. **Si Patricia tiene GPU y Hern√°n no:**
   - Opci√≥n A: Compartir servidor Ollama (Patricia corre Ollama, Hern√°n se conecta remoto)
   - Opci√≥n B: Instalar GPU en m√°quina de Hern√°n

2. **Si ambos tienen solo CPU:**
   - Usar modelo cuantizado: `ollama pull llama3.2:3b-q4_0` (m√°s r√°pido, m√≠nima p√©rdida de calidad)

3. **Si RAM es el problema:**
   - Reducir `num_ctx` de 2048 a 1024 en `client.py:184`

---

## 6. üìù ARCHIVOS MODIFICADOS

### Resumen de cambios:

```
backend/app/llm/client.py (l√≠neas 30-193)
‚îú‚îÄ‚îÄ Nuevo system_prompt con protocolo b√°sico
‚îú‚îÄ‚îÄ Par√°metros optimizados (num_ctx, num_predict, temperature)
‚îî‚îÄ‚îÄ Soporte para historial conversacional

telegram_bot.py (l√≠neas 1-128)
‚îú‚îÄ‚îÄ Import de defaultdict y deque
‚îú‚îÄ‚îÄ Variable conversation_history (l√≠nea 21)
‚îú‚îÄ‚îÄ Comando /clear (l√≠nea 43)
‚îú‚îÄ‚îÄ Env√≠o de historial en payload (l√≠nea 63-66)
‚îî‚îÄ‚îÄ Guardado de respuestas en historial (l√≠nea 81)

backend/app/models.py (l√≠neas 8-18)
‚îú‚îÄ‚îÄ Nuevo modelo ConversationMessage
‚îî‚îÄ‚îÄ Campo historial en QueryRequest

backend/app/main.py (l√≠nea 180-186)
‚îî‚îÄ‚îÄ Pasar historial a llm_client.generate_response()
```

---

## 7. ‚ö†Ô∏è NOTAS IMPORTANTES

### a) **L√≠mite de historial:**
- Bot guarda √∫ltimos **10 mensajes** (5 pares)
- LLM usa √∫ltimos **8 mensajes** (4 pares)
- Esto previene sobrecarga de contexto

### b) **Comandos de limpieza:**
- `/start`: Limpia historial + muestra mensaje de bienvenida
- `/clear`: Solo limpia historial (conversaci√≥n contin√∫a)

### c) **Retrocompatibilidad:**
- Si `historial` no se env√≠a ‚Üí funciona como antes (sin memoria)
- No rompe integraciones existentes

### d) **Performance:**
- Historial aumenta levemente el tiempo de respuesta (~5-10%)
- Los par√°metros optimizados compensan este costo

---

## 8. üöÄ PR√ìXIMOS PASOS (OPCIONAL)

### Mejoras adicionales para considerar:

1. **Modelo m√°s grande:**
   - `llama3.2:7b` ‚Üí Mejor razonamiento, menos hallucinations
   - Requiere: 8GB+ RAM, GPU recomendada

2. **Fine-tuning:**
   - Entrenar llama3.2 con datos reales del hospital
   - Requiere: Dataset de conversaciones reales

3. **Feedback del usuario:**
   - Botones üëç/üëé en Telegram
   - Guardar respuestas problem√°ticas para an√°lisis

4. **M√©tricas:**
   - Log de tiempos de respuesta
   - Obras sociales m√°s consultadas
   - Queries m√°s frecuentes

---

## 9. ‚úÖ CHECKLIST DE VALIDACI√ìN

Antes de considerar terminado:

- [ ] Reiniciar FastAPI y bot de Telegram
- [ ] Probar los 6 tests recomendados
- [ ] Ejecutar comandos de diagn√≥stico en ambas m√°quinas
- [ ] Comparar tiempos de respuesta (antes vs despu√©s)
- [ ] Verificar que `/clear` funciona correctamente
- [ ] Confirmar que no inventa obras sociales
- [ ] Validar que usa protocolo b√°sico en respuestas

---

**Autor:** Claude Sonnet 4.5
**Proyecto:** Agente Hospitalario - Grupo Pedi√°trico
**Versi√≥n:** 0.2.0 (con memoria conversacional)
